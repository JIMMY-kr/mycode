{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA(Latent Dirichlet Allocation, 잠재 디리클레 할당)\n",
    "# 주제별 단어 분포를 바탕으로 주어진 문서에서 발견된 단어수 분포를 분석\n",
    "# 해당 문서의 주제들을 예측하는 기업\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "#영어 단어의 어근만 추출\n",
    "stm = PorterStemmer()\n",
    "\n",
    "#영어 단어의 불용어 집합\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "#특수문자를 제거하기 위한 정규식\n",
    "# 첫 글자는 알파벳으로 시작하고 그 뒤에 영문자 대소문자, 숫자, -,_,.만 허용\n",
    "pattern = re.compile('[a-zA-Z][-_a-zA-Z0-9.]*')\n",
    "\n",
    "#문장을 단어 단위로 분리하고 불용어 및 특수문자 제거 후 어근만 추출하여 list로 반환\n",
    "def tokenize(sentence):\n",
    "    def stem(w):\n",
    "        try: return stm.stem(w)\n",
    "        except: return w\n",
    "        #소문자로 바꾼 후 단어 구분, 불용어 제거, 패턴에 맞는 단어만 선택\n",
    "    return [stem(w) for w in word_tokenize(sentence.lower())\n",
    "    if w not in stopwords and pattern.match(w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs: 1\n",
      "Total words: 162\n",
      "Vocab size: 21\n",
      "Topic #0\teveri, back, mani, america, american, nation, countri, peopl, one, protect\n",
      "Topic #1\tamerica, american, nation, countri, peopl, one, everi, protect, world, great\n",
      "Topic #2\tamerica, american, nation, countri, peopl, one, everi, protect, world, great\n",
      "Topic #3\tworld, across, god, america, american, nation, countri, peopl, one, everi\n",
      "Topic #4\tamerica, american, nation, countri, peopl, one, everi, protect, world, great\n",
      "Topic #5\tmake, america, american, nation, countri, peopl, one, everi, protect, world\n",
      "Topic #6\tnation, peopl, one, america, american, countri, everi, protect, world, great\n",
      "Topic #7\tcountri, protect, america, american, nation, peopl, one, everi, world, great\n",
      "Topic #8\tamerica, american, nation, countri, peopl, one, everi, protect, world, great\n",
      "Topic #9\tamerica, american, nation, countri, peopl, one, everi, protect, world, great\n",
      "Topic #10\tamerica, american, nation, countri, peopl, one, everi, protect, world, great\n",
      "Topic #11\tamerica, american, nation, countri, peopl, one, everi, protect, world, great\n",
      "Topic #12\tgreat, new, america, american, nation, countri, peopl, one, everi, protect\n",
      "Topic #13\tamerica, american, nation, countri, peopl, one, everi, protect, world, great\n",
      "Topic #14\tamerican, dream, today, right, america, nation, countri, peopl, one, everi\n",
      "Topic #15\tamerica, american, nation, countri, peopl, one, everi, protect, world, great\n",
      "Topic #16\tamerica, never, american, nation, countri, peopl, one, everi, protect, world\n",
      "Topic #17\tamerica, american, nation, countri, peopl, one, everi, protect, world, great\n",
      "Topic #18\tprotect, presid, america, american, nation, countri, peopl, one, everi, world\n",
      "Topic #19\tamerica, american, nation, countri, peopl, one, everi, protect, world, great\n"
     ]
    }
   ],
   "source": [
    "import tomotopy as tp\n",
    "\n",
    "#LDAModel 생성\n",
    "# 잠재 디클레리 할당\n",
    "# 주어진 문서에 대해 각 문서에 어떤 주자게 존재하는가를 서술하는 확률적 토픽 모델 기법\n",
    "model = tp.LDAModel(k=20, min_cf=5)\n",
    "\n",
    "#파일에서 한 줄씩 읽어와서 model에 추가\n",
    "for i, line in enumerate(open('c:/data/text/trumph.txt',\n",
    "encoding='ms949')):\n",
    "    model.add_doc(tokenize(line)) #공백을 기준으로 단어를 나누어 model에 추가\n",
    "\n",
    "#train(0): 0회 학습, model의 num_words, num_vocabs 값을 확인하기 위해\n",
    "#실제로 학습은 하지 않고 학습 준비만 하는 상태\n",
    "\n",
    "model.train(0)\n",
    "print('Total docs:', len(model.docs))\n",
    "print('Total words:', model.num_words)\n",
    "print('Vocab size:', model.num_vocabs)\n",
    "model.train(200) # 200회 학습\n",
    "for i in range(model.k):\n",
    "    res = model.get_topic_words(i, top_n=10) #토픽 별 상위 10개 단어\n",
    "    print('Topic #{0}'.format(i), end='\\t')\n",
    "    print(', '.join(w for w, p in res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "emma_raw = nltk.corpus.gutenberg.raw('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'jane',\n",
       " 'austen',\n",
       " 'volum',\n",
       " 'chapter',\n",
       " 'emma',\n",
       " 'woodhous',\n",
       " 'handsom',\n",
       " 'clever',\n",
       " 'rich',\n",
       " 'comfort',\n",
       " 'home',\n",
       " 'happi',\n",
       " 'disposit',\n",
       " 'seem',\n",
       " 'unit',\n",
       " 'best',\n",
       " 'bless',\n",
       " 'exist',\n",
       " 'live',\n",
       " 'nearli',\n",
       " 'twenty-on',\n",
       " 'year',\n",
       " 'world',\n",
       " 'littl',\n",
       " 'distress',\n",
       " 'vex',\n",
       " 'youngest',\n",
       " 'two',\n",
       " 'daughter',\n",
       " 'affection',\n",
       " 'indulg',\n",
       " 'father',\n",
       " 'consequ',\n",
       " 'sister',\n",
       " 'marriag',\n",
       " 'mistress',\n",
       " 'hous',\n",
       " 'earli',\n",
       " 'period',\n",
       " 'mother',\n",
       " 'die',\n",
       " 'long',\n",
       " 'ago',\n",
       " 'indistinct',\n",
       " 'remembr',\n",
       " 'caress',\n",
       " 'place',\n",
       " 'suppli',\n",
       " 'excel',\n",
       " 'woman',\n",
       " 'gover',\n",
       " 'fallen',\n",
       " 'littl',\n",
       " 'short',\n",
       " 'mother',\n",
       " 'affect',\n",
       " 'sixteen',\n",
       " 'year',\n",
       " 'miss',\n",
       " 'taylor',\n",
       " 'mr.',\n",
       " 'woodhous',\n",
       " 'famili',\n",
       " 'less',\n",
       " 'gover',\n",
       " 'friend',\n",
       " 'fond',\n",
       " 'daughter',\n",
       " 'particularli',\n",
       " 'emma',\n",
       " 'intimaci',\n",
       " 'sister',\n",
       " 'even',\n",
       " 'miss',\n",
       " 'taylor',\n",
       " 'ceas',\n",
       " 'hold',\n",
       " 'nomin',\n",
       " 'offic',\n",
       " 'gover',\n",
       " 'mild',\n",
       " 'temper',\n",
       " 'hardli',\n",
       " 'allow',\n",
       " 'impos',\n",
       " 'restraint',\n",
       " 'shadow',\n",
       " 'author',\n",
       " 'long',\n",
       " 'pass',\n",
       " 'away',\n",
       " 'live',\n",
       " 'togeth',\n",
       " 'friend',\n",
       " 'friend',\n",
       " 'mutual',\n",
       " 'attach',\n",
       " 'emma',\n",
       " 'like']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(emma_raw)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs: 1\n",
      "Total words: 67219\n",
      "Vocab size: 1775\n"
     ]
    }
   ],
   "source": [
    "model = tp.LDAModel(k=5, min_cf=5) # 토픽 모델링함수\n",
    "model.add_doc(tokenize(emma_raw))\n",
    "model.train(0)\n",
    "print('Total docs:', len(model.docs))\n",
    "print('Total words:', model.num_words)\n",
    "print('Vocab size:', model.num_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weston', 0.024195490404963493), ('elton', 0.024074817076325417)]\n",
      "Topic #0\tweston, elton\n",
      "[('emma', 0.05767618119716644), ('must', 0.04423421993851662)]\n",
      "Topic #1\temma, must\n",
      "[('mr.', 0.05470478534698486), ('think', 0.027563318610191345)]\n",
      "Topic #2\tmr., think\n",
      "[('mr.', 0.033242397010326385), ('would', 0.02983599342405796)]\n",
      "Topic #3\tmr., would\n",
      "[('much', 0.03445718437433243), ('know', 0.03428183123469353)]\n",
      "Topic #4\tmuch, know\n"
     ]
    }
   ],
   "source": [
    "model.train(100)\n",
    "for i in range(model.k):\n",
    "    res = model.get_topic_words(i, top_n=2)\n",
    "    print(res)\n",
    "    print('Topic #{0}'.format(i), end='\\t')\n",
    "    print(', '.join(w for w, p in res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs: 7\n",
      "Total words: 62\n",
      "Vocab size: 18\n",
      "Topic #0\t민정수석, 논란, 수석, 청와대, 사의, 수용, 대통령, 아들, 입사지원서, 이날\n",
      "Topic #1\t입사지원서, 문재, 이날, 수석, 청와대, 사의, 수용, 민정수석, 대통령, 아들\n",
      "Topic #2\t사의, 표명, 출근, 수석, 청와대, 수용, 민정수석, 대통령, 아들, 입사지원서\n",
      "Topic #3\t아들, 김진국, 수석, 청와대, 사의, 수용, 민정수석, 대통령, 입사지원서, 논란\n",
      "Topic #4\t수용, 이날, 대통령의, 수석, 청와대, 사의, 민정수석, 대통령, 아들, 입사지원서\n",
      "Topic #5\t수석, 청와대, 대통령, 참석, 사의, 수용, 민정수석, 아들, 입사지원서, 논란\n",
      "Topic #6\t수석, 청와대, 사의, 수용, 민정수석, 대통령, 아들, 입사지원서, 논란, 이날\n",
      "Topic #7\t수석, 청와대, 사의, 수용, 민정수석, 대통령, 아들, 입사지원서, 논란, 이날\n",
      "Topic #8\t기자들, 관계자, 수석, 청와대, 사의, 수용, 민정수석, 대통령, 아들, 입사지원서\n",
      "Topic #9\t수석, 청와대, 사의, 수용, 민정수석, 대통령, 아들, 입사지원서, 논란, 이날\n"
     ]
    }
   ],
   "source": [
    "#LDA(한글뉴스)\n",
    "import tomotopy as tp\n",
    "import re\n",
    "from konlpy.tag import Hannanum\n",
    "\n",
    "han = Hannanum()\n",
    "\n",
    "model = tp.LDAModel(k = 10, min_cf=2)\n",
    "\n",
    "#파일에서 한 줄씩 읽어와서 model 추가\n",
    "for i, line in enumerate(open('c:/data/text/news1.txt', encoding='utf-8')):\n",
    "    sentence = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z]', ' ', line)\n",
    "    a=sentence.strip()\n",
    "    n = han.nouns(a)\n",
    "    n2 = [x for x in n if len(x) > 1]\n",
    "    if len(n2) > 0:\n",
    "        model.add_doc(n2)\n",
    "\n",
    "model.train(0)\n",
    "print('Total docs:', len(model.docs))\n",
    "print('Total words:', model.num_words)\n",
    "print('Vocab size:', model.num_vocabs)\n",
    "\n",
    "model.train(200)\n",
    "for i in range(model.k):\n",
    "    res = model.get_topic_words(i, top_n=10)\n",
    "    print('Topic #{0}'.format(i), end='\\t')\n",
    "    print(', '.join(w for w, p in res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#감성분석 : 텍스트에 나타난 주간적 요소를 분석하여 긍정, 부정의 요소 및 그 정도를 판별하여 정량화 하는 기법\n",
    "import glob\n",
    "from afinn import Afinn\n",
    "\n",
    "pos_review = (glob.glob('c:/data/imdb/train/pos/*.txt'))[20]\n",
    "\n",
    "f = open(pos_review, 'r')\n",
    "lines1 = f.readlines()[0]\n",
    "f.close()\n",
    "\n",
    "afinn = Afinn()\n",
    "afinn.score(lines1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:/data/imdb/train/pos\\\\0_9.txt',\n",
       " 'c:/data/imdb/train/pos\\\\10000_8.txt',\n",
       " 'c:/data/imdb/train/pos\\\\10001_10.txt',\n",
       " 'c:/data/imdb/train/pos\\\\10002_7.txt',\n",
       " 'c:/data/imdb/train/pos\\\\10003_8.txt',\n",
       " 'c:/data/imdb/train/pos\\\\10004_8.txt',\n",
       " 'c:/data/imdb/train/pos\\\\10005_7.txt',\n",
       " 'c:/data/imdb/train/pos\\\\10006_7.txt',\n",
       " 'c:/data/imdb/train/pos\\\\10007_7.txt',\n",
       " 'c:/data/imdb/train/pos\\\\10008_7.txt']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = list(glob.glob('c:/data/imdb/train/pos/*.txt')[:10])\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "2.0\n",
      "19.0\n",
      "3.0\n",
      "14.0\n",
      "8.0\n",
      "22.0\n",
      "28.0\n",
      "13.0\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "#학습용 긍정리뷰 10개 파일만 테스트\n",
    "afinn = Afinn() #감성분석 함수\n",
    "for i in files:\n",
    "    f = open(i)\n",
    "    lines1=f.readlines()[0]\n",
    "    print(afinn.score(lines1))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#부정리뷰데이터 20번째 내용\n",
    "neg_review = (glob.glob('c:/data/imdb/train/neg/*.txt')[20])\n",
    "\n",
    "f = open(neg_review, 'r')\n",
    "lines2 = f.readlines()[0]\n",
    "f.close()\n",
    "\n",
    "afinn.score(lines2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:/data/imdb/train/neg\\\\0_3.txt',\n",
       " 'c:/data/imdb/train/neg\\\\10000_4.txt',\n",
       " 'c:/data/imdb/train/neg\\\\10001_4.txt',\n",
       " 'c:/data/imdb/train/neg\\\\10002_1.txt',\n",
       " 'c:/data/imdb/train/neg\\\\10003_1.txt',\n",
       " 'c:/data/imdb/train/neg\\\\10004_3.txt',\n",
       " 'c:/data/imdb/train/neg\\\\10005_3.txt',\n",
       " 'c:/data/imdb/train/neg\\\\10006_4.txt',\n",
       " 'c:/data/imdb/train/neg\\\\10007_1.txt',\n",
       " 'c:/data/imdb/train/neg\\\\10008_2.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = list(glob.glob('c:/data/imdb/train/neg/*.txt')[:10])\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "-4.0\n",
      "9.0\n",
      "5.0\n",
      "-7.0\n",
      "1.0\n",
      "13.0\n",
      "4.0\n",
      "7.0\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "afinn = Afinn()\n",
    "for i in files:\n",
    "    f = open(i)\n",
    "    lines1 = f.readlines()[0]\n",
    "    print(afinn.score(lines1))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#기계학습으로 감성분석\n",
    "#긍정 텍스트 로딩\n",
    "pos_review=(glob.glob('c:/data/imdb/train/pos/*.txt')[:100])\n",
    "\n",
    "lines_pos = []\n",
    "for i in pos_review:\n",
    "    try:\n",
    "        f = open(i, 'r')\n",
    "        temp = f.readlines()[0]\n",
    "        lines_pos.append(temp)\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "len(lines_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#부정 텍스트 로딩\n",
    "neg_review=(glob.glob('c:/data/imdb/train/neg/*.txt')[:100])\n",
    "\n",
    "lines_neg=[]\n",
    "for i in neg_review:\n",
    "    try:\n",
    "        f = open(i, 'r')\n",
    "        temp = f.readlines()[0]\n",
    "        lines_neg.append(temp)\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "len(lines_neg)\n",
    "#긍정 부정 리뷰를 합침\n",
    "total_text = lines_pos + lines_neg\n",
    "\n",
    "len(total_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "x = np.array(['pos', 'neg'])\n",
    "class_Index = np.repeat(x, [len(lines_pos), len(lines_neg)], axis=0)\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어들이 tfidf 가중치를 부여한 후 문서-단어 매트릭스로 바꿈\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer(stop_words=stop_words).fit(total_text)\n",
    "X_train_vectorized = vect.transform(total_text)\n",
    "X_train_vectorized.index = class_Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bromwell</th>\n",
       "      <th>high</th>\n",
       "      <th>cartoon</th>\n",
       "      <th>comedy</th>\n",
       "      <th>ran</th>\n",
       "      <th>time</th>\n",
       "      <th>programs</th>\n",
       "      <th>school</th>\n",
       "      <th>life</th>\n",
       "      <th>teachers</th>\n",
       "      <th>...</th>\n",
       "      <th>zombified</th>\n",
       "      <th>auteur</th>\n",
       "      <th>ample</th>\n",
       "      <th>opportunities</th>\n",
       "      <th>golden</th>\n",
       "      <th>geist</th>\n",
       "      <th>uttered</th>\n",
       "      <th>downloading</th>\n",
       "      <th>midget</th>\n",
       "      <th>tricking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6530 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bromwell  high  cartoon  comedy  ran  time  programs  school  life  \\\n",
       "0       0.0   0.0      0.0     0.0  0.0   0.0       0.0     0.0   0.0   \n",
       "1       0.0   0.0      0.0     0.0  0.0   0.0       0.0     0.0   0.0   \n",
       "2       0.0   0.0      0.0     0.0  0.0   0.0       0.0     0.0   0.0   \n",
       "3       0.0   0.0      0.0     0.0  0.0   0.0       0.0     0.0   0.0   \n",
       "4       0.0   0.0      0.0     0.0  0.0   0.0       0.0     0.0   0.0   \n",
       "\n",
       "   teachers  ...  zombified  auteur  ample  opportunities  golden  geist  \\\n",
       "0       0.0  ...        0.0     0.0    0.0            0.0     0.0    0.0   \n",
       "1       0.0  ...        0.0     0.0    0.0            0.0     0.0    0.0   \n",
       "2       0.0  ...        0.0     0.0    0.0            0.0     0.0    0.0   \n",
       "3       0.0  ...        0.0     0.0    0.0            0.0     0.0    0.0   \n",
       "4       0.0  ...        0.0     0.0    0.0            0.0     0.0    0.0   \n",
       "\n",
       "   uttered  downloading  midget  tricking  \n",
       "0      0.0          0.0     0.0       0.0  \n",
       "1      0.0          0.0     0.0       0.0  \n",
       "2      0.0          0.0     0.0       0.0  \n",
       "3      0.0          0.0     0.0       0.0  \n",
       "4      0.0          0.0     0.0       0.0  \n",
       "\n",
       "[5 rows x 6530 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#데이터프레임으로 변환\n",
    "df = pd.DataFrame(X_train_vectorized.toarray(), columns=vect.vocabulary_.keys())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=10)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#로지스틱 회귀 모형\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logit = LogisticRegression(random_state=10)\n",
    "logit.fit(X_train_vectorized, class_Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#긍정 리뷰들을 하나씩 불러와서 실험\n",
    "def pos_review(model):\n",
    "    count_all = 0\n",
    "    count = 0\n",
    "    num = 100\n",
    "    tests1 = []\n",
    "    for idx in range(0, num):\n",
    "        pos_review_test = (glob.glob(\n",
    "            'c:/data/imdb/test/pos/*.txt'\n",
    "        ))[idx]\n",
    "\n",
    "        f = open(pos_review_test, 'r', encoding='utf-8')\n",
    "        tests1.append(f.readlines())\n",
    "        f.close()\n",
    "    \n",
    "    for test in tests1:\n",
    "        preds = model.predict(vect.transform(test))\n",
    "        result = preds[0]\n",
    "        if result == 'pos':\n",
    "            count += 1\n",
    "        count_all += 1\n",
    "    \n",
    "    rate = count*100 / count_all\n",
    "    print('예측정확도:{0:.1f}%'.format(rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#부정 리뷰들을 하나씩 불러와서 실험\n",
    "def neg_review(model):\n",
    "    count_all = 0\n",
    "    count = 0\n",
    "    num = 100\n",
    "    tests2 = []\n",
    "    for idx in range(0, num):\n",
    "        neg_review_test = (glob.glob(\n",
    "            'c:/data/imdb/test/neg/*.txt'\n",
    "        ))[idx]\n",
    "\n",
    "        f = open(neg_review_test, 'r', encoding='utf-8')\n",
    "        tests2.append(f.readlines())\n",
    "        f.close()\n",
    "    \n",
    "    for test in tests2:\n",
    "        preds = model.predict(vect.transform(test))\n",
    "        result = preds[0]\n",
    "        if result == 'neg':\n",
    "            count += 1\n",
    "        count_all += 1\n",
    "    \n",
    "    rate = count*100 / count_all\n",
    "    print('예측정확도:{0:.1f}%'.format(rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측정확도:66.0%\n",
      "예측정확도:81.0%\n"
     ]
    }
   ],
   "source": [
    "pos_review(logit)\n",
    "neg_review(logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=10)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#의사결정나무\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(random_state=10)\n",
    "tree.fit(X_train_vectorized, class_Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측정확도:39.0%\n",
      "예측정확도:66.0%\n"
     ]
    }
   ],
   "source": [
    "pos_review(tree)\n",
    "neg_review(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=10, random_state=10)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#랜덤포레스트\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#10개의 트리로 구성된 랜덤 포레스트\n",
    "forest = RandomForestClassifier(n_estimators=10, random_state=10)\n",
    "forest.fit(X_train_vectorized, class_Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측정확도:44.0%\n",
      "예측정확도:75.0%\n"
     ]
    }
   ],
   "source": [
    "pos_review(forest)\n",
    "neg_review(forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "knn.fit(X_train_vectorized, class_Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측정확도:34.0%\n",
      "예측정확도:85.0%\n"
     ]
    }
   ],
   "source": [
    "pos_review(knn)\n",
    "neg_review(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(random_state=10)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#인공신경망\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(random_state=10)\n",
    "mlp.fit(X_train_vectorized, class_Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측정확도:58.0%\n",
      "예측정확도:79.0%\n"
     ]
    }
   ],
   "source": [
    "pos_review(mlp)\n",
    "neg_review(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(random_state=10)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVM\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(random_state=10)\n",
    "svm.fit(X_train_vectorized, class_Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측정확도:63.0%\n",
      "예측정확도:87.0%\n"
     ]
    }
   ],
   "source": [
    "pos_review(svm)\n",
    "neg_review(svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ko'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "#언어탐지\n",
    "a = TextBlob('파이썬은 재미있다.')\n",
    "a.detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = TextBlob('I like python.')\n",
    "b.detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Python is fun.\")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#언어 번역 기능\n",
    "c = TextBlob('파이썬은 재미있다.')\n",
    "c.translate(to='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "\n",
    "train = [\n",
    "    ('I love this sandwich', 'pos'),\n",
    "    ('This is an amazing place!', 'pos'),\n",
    "    ('I feel very good about these beers.', 'pos'),\n",
    "    ('This is my best work.', 'pos'),\n",
    "    ('What an awesome view.', 'pos'),\n",
    "    ('I do not like this restaurant.', 'neg'),\n",
    "    ('I am tired of this stuff.', 'neg'),\n",
    "    (\"I can't deal with this.\", 'neg'),\n",
    "    ('He is my sworn enemy!', 'neg'),\n",
    "    ('My boss is horrible.', 'neg')\n",
    "]\n",
    "\n",
    "test = [\n",
    "    ('The beer was good.', 'pos'),\n",
    "    ('I do not enjoy my job.', 'neg'),\n",
    "    (\"I ain't feeling dandy today.\", 'neg'),\n",
    "    (\"I feel amazing!\", 'pos'),\n",
    "    ('Gary is a friend of mine.', 'pos'),\n",
    "    (\"I can't believe I'm doing this.\", 'neg')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = NaiveBayesClassifier(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n",
      "neg\n"
     ]
    }
   ],
   "source": [
    "print(cl.classify(\"Their burgers are amazing\"))\n",
    "print(cl.classify(\"I don't like their pizza.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#여러 문장을 종합하여 부정으로 분류\n",
    "blob = TextBlob('The beer was amazing. But the hangover was horrible. My boss was not happy.', classifier=cl)\n",
    "blob.classify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The beer was amazing. ==> pos\n",
      "But the hangover was horrible. ==> neg\n",
      "My boss was not happy. ==> neg\n"
     ]
    }
   ],
   "source": [
    "#개별 문장으로 분류\n",
    "for sentence in blob.sentences:\n",
    "    print(sentence, '==>', sentence.classify()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The beer was good. ==> neg\n",
      "I do not enjoy my job. ==> neg\n",
      "I ain't feeling dandy today. ==> neg\n",
      "I feel amazing! ==> neg\n",
      "Gary is a friend of mine. ==> neg\n",
      "I can't believe I'm doing this. ==> neg\n"
     ]
    }
   ],
   "source": [
    "for row in test:\n",
    "    print(row[0], '==>', sentence.classify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.accuracy(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "          contains(this) = True              neg : pos    =      2.3 : 1.0\n",
      "          contains(this) = False             pos : neg    =      1.8 : 1.0\n",
      "          contains(This) = False             neg : pos    =      1.6 : 1.0\n",
      "            contains(an) = False             neg : pos    =      1.6 : 1.0\n",
      "             contains(I) = False             pos : neg    =      1.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "cl.show_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#연관어 분석\n",
    "#동시출현빈도 기반\n",
    "import glob\n",
    "# 긍정리뷰 100개 로딩\n",
    "pos_review = (glob.glob('c:/data/imdb/train/pos/*.txt'))[0:100]\n",
    "lines_pos=[]\n",
    "for i in pos_review:\n",
    "    try:\n",
    "        f = open(i, 'r')\n",
    "        temp = f.readlines()[0]\n",
    "        lines_pos.append(temp)\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "len(lines_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terms</th>\n",
       "      <th>terms2</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>film</td>\n",
       "      <td>story</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>movie</td>\n",
       "      <td>one</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>film</td>\n",
       "      <td>movie</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>movie</td>\n",
       "      <td>story</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>one</td>\n",
       "      <td>story</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>good</td>\n",
       "      <td>movie</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>film</td>\n",
       "      <td>one</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>movie</td>\n",
       "      <td>see</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>one</td>\n",
       "      <td>see</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>film</td>\n",
       "      <td>like</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>film</td>\n",
       "      <td>good</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>like</td>\n",
       "      <td>movie</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>good</td>\n",
       "      <td>story</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>great</td>\n",
       "      <td>movie</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>film</td>\n",
       "      <td>time</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>film</td>\n",
       "      <td>well</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>movie</td>\n",
       "      <td>much</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>movie</td>\n",
       "      <td>time</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>good</td>\n",
       "      <td>one</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>film</td>\n",
       "      <td>see</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    terms terms2  freq\n",
       "0    film  story    41\n",
       "1   movie    one    41\n",
       "2    film  movie    35\n",
       "3   movie  story    35\n",
       "4     one  story    33\n",
       "5    good  movie    32\n",
       "6    film    one    31\n",
       "7   movie    see    30\n",
       "8     one    see    27\n",
       "9    film   like    27\n",
       "10   film   good    26\n",
       "11   like  movie    26\n",
       "12   good  story    26\n",
       "13  great  movie    26\n",
       "14   film   time    25\n",
       "15   film   well    25\n",
       "16  movie   much    25\n",
       "17  movie   time    25\n",
       "18   good    one    25\n",
       "19   film    see    24"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = RegexpTokenizer('[\\w]+')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#동시출현 단어 계산\n",
    "count = {} #동시출현 빈도가 저장될 dict\n",
    "for line in lines_pos:\n",
    "    words = line.lower()\n",
    "    tokens = tokenizer.tokenize(words)\n",
    "    #불용어 제거, 불용어에 br추가\n",
    "    stopped_tokens = [i for i in list(set(tokens)) if i not in stop_words+['br']]\n",
    "    #글자수가 1인 단어 제외\n",
    "    stopped_tokens2 = [i for i in stopped_tokens if len(i) > 1]\n",
    "    for i, a in enumerate(stopped_tokens2):\n",
    "        for b in stopped_tokens2[i+1:]:\n",
    "            if a > b:\n",
    "                count[b,a] = count.get((b,a),0)+1\n",
    "            else:\n",
    "                count[a,b] = count.get((a,b),0)+1\n",
    "\n",
    "df = pd.DataFrame.from_dict(count, orient='index')\n",
    "#리스트 구성\n",
    "list1 = []\n",
    "for i in range(len(df)):\n",
    "    list1.append([df.index[i][0], df.index[i][1], df[0][i]])\n",
    "\n",
    "df2 = pd.DataFrame(list1, columns=['terms', 'terms2', 'freq'])\n",
    "df3 = df2.sort_values(by=['freq'], ascending=False)\n",
    "df3_pos = df3.reset_index(drop=True)\n",
    "\n",
    "#동시출현 단어 페어 빈도 상위 20개 출력\n",
    "df3_pos.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#부정 리뷰에 적용\n",
    "neg_review=(glob.glob('c:/data/imdb/train/neg/*.txt'))[0:100]\n",
    "\n",
    "lines_neg=[]\n",
    "for i in neg_review:\n",
    "    try:\n",
    "        f = open(i, 'r')\n",
    "        temp = f.readlines()[0]\n",
    "        lines_neg.append(temp)\n",
    "        f.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "len(lines_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terms</th>\n",
       "      <th>terms2</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>film</td>\n",
       "      <td>movie</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>like</td>\n",
       "      <td>movie</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>movie</td>\n",
       "      <td>one</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>film</td>\n",
       "      <td>one</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>like</td>\n",
       "      <td>one</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>even</td>\n",
       "      <td>movie</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>good</td>\n",
       "      <td>movie</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>even</td>\n",
       "      <td>like</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>good</td>\n",
       "      <td>one</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>film</td>\n",
       "      <td>like</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>film</td>\n",
       "      <td>good</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>movie</td>\n",
       "      <td>would</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>good</td>\n",
       "      <td>like</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>movie</td>\n",
       "      <td>much</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>film</td>\n",
       "      <td>would</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>even</td>\n",
       "      <td>film</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>film</td>\n",
       "      <td>really</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bad</td>\n",
       "      <td>one</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>even</td>\n",
       "      <td>one</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>even</td>\n",
       "      <td>good</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    terms  terms2  freq\n",
       "0    film   movie    42\n",
       "1    like   movie    40\n",
       "2   movie     one    38\n",
       "3    film     one    35\n",
       "4    like     one    33\n",
       "5    even   movie    32\n",
       "6    good   movie    32\n",
       "7    even    like    31\n",
       "8    good     one    30\n",
       "9    film    like    29\n",
       "10   film    good    28\n",
       "11  movie   would    27\n",
       "12   good    like    27\n",
       "13  movie    much    26\n",
       "14   film   would    25\n",
       "15   even    film    25\n",
       "16   film  really    25\n",
       "17    bad     one    25\n",
       "18   even     one    25\n",
       "19   even    good    25"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = {} #동시출현 빈도가 저장될 dict\n",
    "for line in lines_neg:\n",
    "    words = line.lower()\n",
    "    tokens = tokenizer.tokenize(words)\n",
    "    stopped_tokens = [i for i in list(set(tokens)) if i not in stop_words+['br']]\n",
    "    stopped_tokens2 = [i for i in stopped_tokens if len(i) > 1]\n",
    "    for i, a in enumerate(stopped_tokens2):\n",
    "        for b in stopped_tokens2[i+1:]:\n",
    "            if a > b:\n",
    "                count[b,a] = count.get((b,a),0)+1\n",
    "            else:\n",
    "                count[a,b] = count.get((a,b),0)+1\n",
    "\n",
    "df = pd.DataFrame.from_dict(count, orient='index')\n",
    "#리스트 구성\n",
    "list1 = []\n",
    "for i in range(len(df)):\n",
    "    list1.append([df.index[i][0], df.index[i][1], df[0][i]])\n",
    "\n",
    "df2 = pd.DataFrame(list1, columns=['terms', 'terms2', 'freq'])\n",
    "df3 = df2.sort_values(by=['freq'], ascending=False)\n",
    "df3_neg = df3.reset_index(drop=True)\n",
    "\n",
    "#동시출현 단어 페어 빈도 상위 20개 출력\n",
    "df3_neg.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#통계적 가중치 기반\n",
    "import glob\n",
    "\n",
    "#긍정리뷰 100개\n",
    "pos_review=(glob.glob('c:/data/imdb/train/pos/*.txt'))[0:100]\n",
    "\n",
    "lines_pos = []\n",
    "for i in pos_review:\n",
    "    try:\n",
    "        f = open(i, 'r')\n",
    "        temp = f.readlines()[0]\n",
    "        lines_pos.append(temp)\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "len(lines_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4001)\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.06538462 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.23078109 0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#전처리\n",
    "tokenizer = RegexpTokenizer('[\\w]+')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#TF-IDF가중치 할당\n",
    "vec = TfidfVectorizer(stop_words=stop_words)\n",
    "vector_lines_pos = vec.fit_transform(lines_pos)\n",
    "A = vector_lines_pos.toarray()\n",
    "print(A.shape)\n",
    "print(A)\n",
    "# x축 단어, y축 문서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4001, 100)\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.06538462 0.23078109]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#현재 : 100개의 문서의 유사도\n",
    "#목적 : 단어간 유사도를 구하는 것\n",
    "#단어-문서 행렬로 변경 >>> x축 문서, y축 단어\n",
    "A =A.transpose()\n",
    "print(A.shape)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.5\n",
      "  (1, 1)\t1.0\n",
      "  (2, 0)\t0.7\n",
      "  (2, 2)\t1.5\n",
      "[[0.5 0.  0. ]\n",
      " [0.  1.  0. ]\n",
      " [0.7 0.  1.5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "#밀집행렬(모든 값이 다 채워져있음) <--> 희소행렬\n",
    "a = np.array([[0.5,0,0],[0,1,0],[0.7,0,1.5]])\n",
    "\n",
    "b = sparse.csr_matrix(a)\n",
    "print('{}'.format(b))\n",
    "c = b.toarray() #다시 밀집행렬로 변환\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1469, 108), 0.37803585968894865),\n",
       " ((1470, 108), 0.2189685434746738),\n",
       " ((1476, 108), 0.06407477897013734),\n",
       " ((1477, 108), 0.185189577514238),\n",
       " ((1480, 108), 0.20111036876169444),\n",
       " ((1489, 108), 0.06995711757772019),\n",
       " ((1496, 108), 0.10714874067068783),\n",
       " ((1503, 108), 0.30487333830091773),\n",
       " ((1504, 108), 0.30487333830091773),\n",
       " ((1512, 108), 0.30487333830091773)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "A_sparse = sparse.csr_matrix(A)\n",
    "#코사인 유사도 계산\n",
    "similarities_sparse = cosine_similarity(A_sparse, dense_output=False)\n",
    "#todok() 행렬을 딕셔너리 형태로 변환\n",
    "list(similarities_sparse.todok().items())[35000:35010]\n",
    "\n",
    "#에러는 아닙니다. 위에서 코사인 유사도를 계산한 후에 빈 리스트에 입력해서 출력되어야 하는데,\n",
    "# 값들이 입력이 안되서 단어의 인덱스를 확인하지 못하고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fraud'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.get_feature_names()[1469]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'actual'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.get_feature_names()[108]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>(49, 46)</td>\n",
       "      <td>0.364635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>(46, 49)</td>\n",
       "      <td>0.364635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>(50, 53)</td>\n",
       "      <td>0.327337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>(53, 50)</td>\n",
       "      <td>0.327337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>(7, 16)</td>\n",
       "      <td>0.325747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>(16, 7)</td>\n",
       "      <td>0.325747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>(8, 7)</td>\n",
       "      <td>0.323691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>(7, 8)</td>\n",
       "      <td>0.323691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>(53, 42)</td>\n",
       "      <td>0.322940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>(42, 53)</td>\n",
       "      <td>0.322940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        words    weight\n",
       "100  (49, 46)  0.364635\n",
       "101  (46, 49)  0.364635\n",
       "102  (50, 53)  0.327337\n",
       "103  (53, 50)  0.327337\n",
       "104   (7, 16)  0.325747\n",
       "105   (16, 7)  0.325747\n",
       "106    (8, 7)  0.323691\n",
       "107    (7, 8)  0.323691\n",
       "108  (53, 42)  0.322940\n",
       "109  (42, 53)  0.322940"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(list(similarities_sparse.todok().items()), columns=['words', 'weight'])\n",
    "df2 = df.sort_values(by=['weight'], ascending=False)\n",
    "df2 = df2.reset_index(drop=True)\n",
    "\n",
    "df3 = df2.loc[np.round(df2['weight']) < 1]\n",
    "df3.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word2vec 기반\n",
    "# 단어의 의미는 그 단어 주변 단어의 분포로 이해될 수 있음\n",
    "\n",
    "import glob\n",
    "pos_review=(glob.glob('c:/data/imdb/train/pos/*.txt')[0:100])\n",
    "\n",
    "lines_pos=[]\n",
    "for i in pos_review:\n",
    "    try:\n",
    "        f = open(i, 'r')\n",
    "        temp = f.readlines()[0]\n",
    "        lines_pos.append(temp)\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "len(lines_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9341292"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "tokenizer = RegexpTokenizer('[\\w]+')\n",
    "\n",
    "#단어 추출\n",
    "text=[]\n",
    "for line in lines_pos:\n",
    "    words = line.lower()\n",
    "    tokens = tokenizer.tokenize(words)\n",
    "    stopped_tokens = [i for i in list(set(tokens)) if i not in stop_words+['br']]\n",
    "    stopped_tokens2 = [i for i in stopped_tokens if len(i) > 1]\n",
    "    text.append(stopped_tokens2)\n",
    "\n",
    "#word2vec 모형 생성, sg = 1 skip-gram을 적용, window=2 중심 단어로부터 좌우 2개의 단어까지 학습에 적용\n",
    "# min_count=3 최소 3회 이상 출현한 단어들을 대상으로 학습\n",
    "model = Word2Vec(text, vector_size=10, sg=1, window=2, min_count=3)\n",
    "#불필요한 메모리 unload\n",
    "model.wv.similarity('film', 'movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('use', 0.9330725073814392),\n",
       " ('character', 0.9153137803077698),\n",
       " ('think', 0.9129804968833923),\n",
       " ('every', 0.8964542150497437),\n",
       " ('never', 0.8929697871208191)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#good과 가장 유사한 단어 5개\n",
    "model.wv.most_similar('good', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "895"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델에 저장된 단어의 갯수\n",
    "len(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie',\n",
       " 'film',\n",
       " 'story',\n",
       " 'one',\n",
       " 'see',\n",
       " 'good',\n",
       " 'like',\n",
       " 'well',\n",
       " 'time',\n",
       " 'much']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델에 저장된 단어 목록\n",
    "model.wv.index_to_key[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cef9e06bb236b2a8629b07e87a04b187b952a0f661eff5533360a155783f0c33"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
