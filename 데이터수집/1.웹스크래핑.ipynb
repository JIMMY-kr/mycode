{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "html = urlopen('http://google.com')\n",
    "print(html.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "try:\n",
    "    html = urlopen('http://jaba.com')\n",
    "\n",
    "except:\n",
    "    print('url open error')\n",
    "\n",
    "else:\n",
    "    print(html.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "try:\n",
    "    html = urlopen('http://jaba.com')\n",
    "\n",
    "except:\n",
    "    print('url open error')\n",
    "\n",
    "else:\n",
    "    print(html.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "url='https://t1.daumcdn.net/daumtop_chanel/op/20170315064553027.png'\n",
    "savename = 'c:/data/images/daum.png'\n",
    "urllib.request.urlretrieve(url, savename)\n",
    "print('file saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savename='c:/data/images/myimg2.jpg'\n",
    "image=urllib.request.urlopen(url).read()\n",
    "with open(savename, mode='wb') as f:\n",
    "    f.write(image)\n",
    "\n",
    "print('file saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "api= \"http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp\"\n",
    "\n",
    "values={'stnId': '108'}\n",
    "params = urllib.parse.urlencode(values)\n",
    "url = api + '?' + params\n",
    "print('url=', url)\n",
    "data = urllib.request.urlopen(url).read()\n",
    "text = data.decode('utf-8')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html=urlopen('http://stackoverflow.com') #url 접속, 오픈\n",
    "bs = BeautifulSoup(html.read(), 'html.parser') # html 분석객체\n",
    "print(bs.title) # html 문서 내부의 title 태그 선택\n",
    "print(bs.h1) # h1 태그 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html='''\n",
    "<html>\n",
    "<body>\n",
    "    <h1>Hello Python</h1>\n",
    "    <p>웹 파이지 분석</p>\n",
    "    <p>웹 스크레이핑</p>\n",
    "</body></html>\n",
    "'''\n",
    "\n",
    "bs=BeautifulSoup(html, 'html.parser') # html 분석 객체\n",
    "h1 = bs.html.body.h1\n",
    "p1 = bs.html.body.p #첫번째 p태그\n",
    "#next_sibling 다음요소\n",
    "p2 = p1.next_sibling.next_sibling\n",
    "print('h1 = ' + h1.text) #태그 포함\n",
    "print('p = ' + p1.text)\n",
    "print('p = ' + p2.text)\n",
    "\n",
    "items=bs.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html='''\n",
    "<html>\n",
    "<body>\n",
    "    <h1 id='title'>Hello Python</h1>\n",
    "    <p id='body'>웹 파이지 분석</p>\n",
    "    <p>웹 스크레이핑</p>\n",
    "    <span>데이터 수집</span>\n",
    "</body></html>\n",
    "'''\n",
    "\n",
    "bs=BeautifulSoup(html, 'html.parser') # html 분석 객체\n",
    "title = bs.find(id='title')\n",
    "body = bs.find(id='body')\n",
    "span = bs.find('span')\n",
    "\n",
    "print('#title=', title.text)\n",
    "print('#body=', body.text)\n",
    "print('#span=', span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html='''\n",
    "<html>\n",
    "<body>\n",
    "<ul>\n",
    "    <li><a href='http://naver.com'>naver</a></li>\n",
    "    <li><a href='http://google.com'>google</a></li>\n",
    "    <li><a href='http://duam.com'>duam</a></li>\n",
    "    <li><a href='http://nate.com'>nate</a></li>\n",
    "    <li><a href='http://yahoo.com'>yahoo</a></li>\n",
    "</ul>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "bs=BeautifulSoup(html, 'html.parser')\n",
    "links=bs.find_all('a') #모든 a 태그 리스트\n",
    "for a in links:\n",
    "    href=a['href'] #태그의 속성값\n",
    "    text=a.text #태그 내부의 텍스트\n",
    "    print(text, ':', href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "category = ['33580000']\n",
    "for cate in category:\n",
    "    html = urlopen('http://browse.auction.co.kr/list?category='+cate)\n",
    "    bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "    tags = bs.find_all('img')\n",
    "    for img in tags:\n",
    "        if 'https' not in img['src']:\n",
    "            src = 'https:' + img['src']\n",
    "        print(src)\n",
    "        name=src.split('/')[-1]\n",
    "        urllib.request.urlretrieve(src, 'c:/data/images/'+name)\n",
    "\n",
    "# robots.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "url = 'http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp'\n",
    "res = req.urlopen(url)\n",
    "bs = BeautifulSoup(res, 'html.parser')\n",
    "title=bs.find('title').text\n",
    "wf=bs.find('wf').text\n",
    "\n",
    "result1=[]\n",
    "result2=[]\n",
    "result3=[]\n",
    "result4=[]\n",
    "result5=[]\n",
    "items=bs.find_all('data')\n",
    "for item in items:\n",
    "    times=item.find_all('tmef')\n",
    "    wfs = item.find_all('wf')\n",
    "    tmns = item.find_all('tmn')\n",
    "    tmxs = item.find_all('tmx')\n",
    "    rnsts = item.find_all('rnst')\n",
    "    for idx in range(len(times)):\n",
    "        result1.append(times[idx].text)\n",
    "        result2.append(wfs[idx].text)\n",
    "        result3.append(tmns[idx].text)\n",
    "        result4.append(tmxs[idx].text)\n",
    "        result5.append(rnsts[idx].text)\n",
    "    \n",
    "print(result1[:5])\n",
    "print(result2[:5])\n",
    "\n",
    "result=zip(result1,result2,result3,result4,result5)\n",
    "rows = list(result)\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(rows,\n",
    "columns=['일시', '날씨', '최저', '최고', '강수확률'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('c:/data/weather/data.csv', encoding='ms949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "pymysql.install_as_MySQLdb()\n",
    "engine=create_engine('mysql+mysqldb://web:1234@localhost/myweb', encoding='utf-8')\n",
    "conn=engine.connect()\n",
    "df.to_sql(name='weather', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html = '''\n",
    "<html><body>\n",
    "<div id='main'>\n",
    "    <h1>도서목록</h1>\n",
    "    <ul class='items'>\n",
    "        <li>빅데이터</li>\n",
    "        <li>HTML</li>\n",
    "        <li>Python</li>\n",
    "    </ul>\n",
    "</div>\n",
    "</body></html>\n",
    "'''\n",
    "\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "h1 = bs.select_one('div#main > h1').text\n",
    "print('h1 =', h1)\n",
    "\n",
    "li_list = bs.select('div#main > ul.items > li')\n",
    "for li in li_list:\n",
    "    print('li =', li.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "url='http://finance.naver.com/'\n",
    "res = req.urlopen(url)\n",
    "bs=BeautifulSoup(res, 'html.parser')\n",
    "\n",
    "val=bs.select_one('a > span > span.num').text\n",
    "print('코스피지수:', val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#content > div.article2 > div.section1 > div.group1 > table > tbody > tr.down.bold > th > a\n",
    "#content > div.article2 > div.section1 > div.group1 > table > tbody > tr.down.bold > td:nth-child(2)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url='http://finance.naver.com'\n",
    "res=req.urlopen(url)\n",
    "bs=BeautifulSoup(res, 'html.parser')\n",
    "\n",
    "result1=[]\n",
    "moneys = bs.select('#content > div.article2 > div.section1 > div.group1 > table > tbody > tr > th > a') #공통된 패턴을 만들기 위해 .down.bold(강조표시)를 제거\n",
    "for money in moneys:\n",
    "    result1.append(money.text.strip()) #strip() 좌우공백제거\n",
    "\n",
    "print(result1)\n",
    "\n",
    "result2=[]\n",
    "exchange = bs.select('#content > div.article2 > div.section1 > div.group1 > table > tbody > tr > td:nth-child(2)')\n",
    "for ex in exchange:\n",
    "    result2.append(ex.text)\n",
    "\n",
    "print(result2)\n",
    "\n",
    "result=list(zip(result1, result2)) #zip 같은 인덱스끼리 결합\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://ko.wikipedia.org/wiki/%EC%9C%A4%EB%8F%99%EC%A3%BC #위키피디아의 윤동주 시인 페이지\n",
    "#mw-content-text > div.mw-parser-output > ul:nth-child(49) > li:nth-child(1)\n",
    "#mw-content-text > div.mw-parser-output > ul:nth-child(49) > li:nth-child(2)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = 'http://ko.wikipedia.org/wiki/%EC%9C%A4%EB%8F%99%EC%A3%BC'\n",
    "res=req.urlopen(url)\n",
    "bs=BeautifulSoup(res, 'html.parser')\n",
    "\n",
    "a_list=bs.select('#mw-content-text > div.mw-parser-output > ul:nth-child(49) > li')\n",
    "for a in a_list:\n",
    "    print(a.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://news.naver.com/\n",
    "#today_main_news > div.hdline_news > ul > li:nth-child(1) > div.hdline_article_tit > a\n",
    "#today_main_news > div.hdline_news > ul > li:nth-child(2) > div.hdline_article_tit > a\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = 'https://news.naver.com/'\n",
    "res=req.urlopen(url)\n",
    "bs=BeautifulSoup(res, 'html.parser')\n",
    "\n",
    "a_list=bs.select('#today_main_news > div.hdline_news > ul > li > div.hdline_article_tit > a')\n",
    "for a in a_list:\n",
    "    print(a.text.strip())\n",
    "    print(a['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import urllib.request as req \n",
    "\n",
    "codes=[]\n",
    "for num in range(51,53): # 51~52\n",
    "    url = \"http://www.auction.co.kr/category/category{0}.html\".format(num)\n",
    "    print(url)\n",
    "    res = req.urlopen(url)\n",
    "    # HTML 분석하기\n",
    "    bs = BeautifulSoup(res, \"html.parser\", from_encoding='ms949')\n",
    "    # 원하는 데이터 추출(카테고리 전체보기를 클릭한 후 개발자 도구에서 패턴 찾기)\n",
    "    category = bs.select(\\\n",
    "  \"div.layer-body > div > div > div > ul > li > ul > li > a\")\n",
    "    for cate in category:\n",
    "        name=cate.text\n",
    "        print(cate[\"href\"])\n",
    "        href=cate[\"href\"].split(\"=\")[1]\n",
    "        codes.append(href)\n",
    "        print(\"상품분류명:\"+name)\n",
    "        print(\"분류코드:\"+href+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://www.auction.co.kr/category/category50.html\n",
    "#region--content_filter > div > div:nth-child(1) > div.component.component--filter.type--toggle.name--category.on > div > div > ul > li:nth-child(1) > a\n",
    "#region--content_filter > div > div:nth-child(1) > div.component.component--filter.type--toggle.name--category.on > div > div > ul > li:nth-child(2) > a\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "\n",
    "codes=[]\n",
    "for num in range(51,53):\n",
    "    url = 'http://www.auction.co.kr/category/category{0}.html'.format(num)\n",
    "    print(url)\n",
    "    res=req.urlopen(url)\n",
    "    bs=BeautifulSoup(res, 'html.parser', from_encoding='ms949')\n",
    "    category = bs.select('div.layer-body > div > div > div > ul > li > ul > li > a')\n",
    "        \n",
    "    for cate in category:\n",
    "        name=cate.text\n",
    "        try:\n",
    "            href=cate.attr['href'].split('=')[1]\n",
    "            codes.append(href)\n",
    "\n",
    "            print('상품분류명:'+name)\n",
    "            print('분류코드:'+href+'\\n')\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cate in codes:\n",
    "    html = urlopen('http://browse.auction.co.kr/list?category='+cate)\n",
    "    bs=BeautifulSoup(html.read(), 'html.parser')\n",
    "    tags = bs.find_all('img')\n",
    "    for img in tags:\n",
    "        if 'http:' not in img['src']:\n",
    "            src='http:' +img['src']\n",
    "        print(src)\n",
    "        name = src.rsplit('/', 1)[-1]\n",
    "        urllib.request.urlretrieve(src, 'c:/data/images/'+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cef9e06bb236b2a8629b07e87a04b187b952a0f661eff5533360a155783f0c33"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
